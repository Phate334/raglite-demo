services:
  llm:
    image: ghcr.io/ggerganov/llama.cpp:server-b3909
    ports:
      - "8080:8080"
    volumes:
      - ./models:/models
    env_file:
      - llm.env
  embedding:
    image: ghcr.io/ggerganov/llama.cpp:server-b3909
    ports:
      - "8081:8080"
    volumes:
      - ./models:/models
    env_file:
      - embedding.env
  proxy:
    image: ghcr.io/berriai/litellm:main-v1.50.4-stable
    ports:
      - "8000:4000"
    volumes:
      - ./litellm.yaml:/app/config.yaml
    command: --config /app/config.yaml
    depends_on:
      - llm
      - embedding
